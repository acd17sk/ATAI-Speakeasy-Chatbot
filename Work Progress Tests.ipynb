{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e40bac0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "import re\n",
    "from word_forms.word_forms import get_word_forms\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import inflect\n",
    "\n",
    "inflect_engine = inflect.engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36671b9",
   "metadata": {},
   "source": [
    "# Downloading Models and storing them locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb861ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe617844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('models/ent_name_sim/', 'ent_name_sim')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac7c83d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagger = SequenceTagger.load(\"flair/pos-english\")\n",
    "# tagger.save('models/pos1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03a97244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner_large = SequenceTagger.load(\"flair/ner-english-large\")\n",
    "# ner_large.save('models/ner_large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90d3f1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner_base = SequenceTagger.load(\"flair/ner-english\")\n",
    "# ner_base.save('models/ner_base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "550034a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tagger = SequenceTagger.load('models/pos1')\n",
    "# ner_large = SequenceTagger.load('models/ner_large')\n",
    "# ner_base = SequenceTagger.load('models/ner_base')\n",
    "# model = SentenceTransformer('models/ent_name_sim/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f46671a",
   "metadata": {},
   "source": [
    "# Filter crowd source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86cc7eb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# crowd_pd = pd.read_csv('Data/crowd_data/crowd_data.tsv', sep='\\t')\n",
    "# crowd_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7873632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crowd_pd_filter = crowd_pd.filter(['HITId','WorkTimeInSeconds', 'WorkerId'],axis=1)\n",
    "# malicious_hits = crowd_pd_filter.loc[crowd_pd_filter.groupby('HITId')['WorkTimeInSeconds'].idxmin()]\n",
    "# malicious_ids = malicious_hits['WorkerId'].unique()\n",
    "# malicious_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9766e1b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# clean_crowd_pd = crowd_pd.drop(crowd_pd[(crowd_pd['WorkerId'].isin(malicious_ids))].index)\n",
    "# clean_crowd_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5ddfbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_crowd_pd.to_csv(\"Data/crowd_data/clean_crowd_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a51105f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans_filter = crowd_pd.filter(['HITId','AnswerLabel'])\n",
    "# ans_count = ans_filter.groupby('HITId')['AnswerLabel'].value_counts('counts').to_frame()\n",
    "# rates = ans_count.groupby('HITId')['AnswerLabel'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8ebc254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load csv module\n",
    "# import json\n",
    "\n",
    "# j = json.dumps(dict(rates))\n",
    "\n",
    "# # open file for writing, \"w\" \n",
    "# f = open(\"Data/crowd_data/rates.json\",\"w\")\n",
    "\n",
    "# # write json object to file\n",
    "# f.write(j)\n",
    "\n",
    "# # close file\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12f972ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>HITId</th>\n",
       "      <th>HITTypeId</th>\n",
       "      <th>Title</th>\n",
       "      <th>Reward</th>\n",
       "      <th>AssignmentId</th>\n",
       "      <th>WorkerId</th>\n",
       "      <th>AssignmentStatus</th>\n",
       "      <th>WorkTimeInSeconds</th>\n",
       "      <th>LifetimeApprovalRate</th>\n",
       "      <th>Input1ID</th>\n",
       "      <th>Input2ID</th>\n",
       "      <th>Input3ID</th>\n",
       "      <th>AnswerID</th>\n",
       "      <th>AnswerLabel</th>\n",
       "      <th>FixPosition</th>\n",
       "      <th>FixValue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7QT</td>\n",
       "      <td>Is this triple correct or incorrect?</td>\n",
       "      <td>$0.50</td>\n",
       "      <td>1</td>\n",
       "      <td>2133ICYWE97</td>\n",
       "      <td>Submitted</td>\n",
       "      <td>60</td>\n",
       "      <td>99%</td>\n",
       "      <td>wd:Q11621</td>\n",
       "      <td>wdt:P2142</td>\n",
       "      <td>792910554</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CORRECT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7QT</td>\n",
       "      <td>Is this triple correct or incorrect?</td>\n",
       "      <td>$0.50</td>\n",
       "      <td>2</td>\n",
       "      <td>2133U7HKDLO</td>\n",
       "      <td>Submitted</td>\n",
       "      <td>40</td>\n",
       "      <td>40%</td>\n",
       "      <td>wd:Q11621</td>\n",
       "      <td>wdt:P2142</td>\n",
       "      <td>792910554</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CORRECT</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7QT</td>\n",
       "      <td>Is this triple correct or incorrect?</td>\n",
       "      <td>$0.50</td>\n",
       "      <td>3</td>\n",
       "      <td>928UJANWZ12</td>\n",
       "      <td>Submitted</td>\n",
       "      <td>50</td>\n",
       "      <td>98%</td>\n",
       "      <td>wd:Q11621</td>\n",
       "      <td>wdt:P2142</td>\n",
       "      <td>792910554</td>\n",
       "      <td>2.0</td>\n",
       "      <td>INCORRECT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7QT</td>\n",
       "      <td>Is this triple correct or incorrect?</td>\n",
       "      <td>$0.50</td>\n",
       "      <td>4</td>\n",
       "      <td>1726JMZQW</td>\n",
       "      <td>Submitted</td>\n",
       "      <td>80</td>\n",
       "      <td>70%</td>\n",
       "      <td>wd:Q11621</td>\n",
       "      <td>wdt:P2142</td>\n",
       "      <td>792910554</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CORRECT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>7QT</td>\n",
       "      <td>Is this triple correct or incorrect?</td>\n",
       "      <td>$0.50</td>\n",
       "      <td>7</td>\n",
       "      <td>2133ICYWE97</td>\n",
       "      <td>Submitted</td>\n",
       "      <td>120</td>\n",
       "      <td>99%</td>\n",
       "      <td>wd:Q603545</td>\n",
       "      <td>wdt:P2142</td>\n",
       "      <td>4300000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CORRECT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>296</td>\n",
       "      <td>60</td>\n",
       "      <td>9QT</td>\n",
       "      <td>Is this triple correct or incorrect?</td>\n",
       "      <td>$0.50</td>\n",
       "      <td>297</td>\n",
       "      <td>HHCKW1111</td>\n",
       "      <td>Submitted</td>\n",
       "      <td>200</td>\n",
       "      <td>80%</td>\n",
       "      <td>wd:Q21060270</td>\n",
       "      <td>wdt:P27</td>\n",
       "      <td>wd:Q916</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CORRECT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>297</td>\n",
       "      <td>60</td>\n",
       "      <td>9QT</td>\n",
       "      <td>Is this triple correct or incorrect?</td>\n",
       "      <td>$0.50</td>\n",
       "      <td>298</td>\n",
       "      <td>GGUI83657S</td>\n",
       "      <td>Submitted</td>\n",
       "      <td>120</td>\n",
       "      <td>85%</td>\n",
       "      <td>wd:Q21060270</td>\n",
       "      <td>wdt:P27</td>\n",
       "      <td>wd:Q916</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CORRECT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>300</td>\n",
       "      <td>61</td>\n",
       "      <td>9QT</td>\n",
       "      <td>Is this triple correct or incorrect?</td>\n",
       "      <td>$0.50</td>\n",
       "      <td>301</td>\n",
       "      <td>AALKMII97</td>\n",
       "      <td>Submitted</td>\n",
       "      <td>240</td>\n",
       "      <td>98%</td>\n",
       "      <td>wd:Q1288004</td>\n",
       "      <td>wdt:P1412</td>\n",
       "      <td>wd:Q13330</td>\n",
       "      <td>2.0</td>\n",
       "      <td>INCORRECT</td>\n",
       "      <td>Object</td>\n",
       "      <td>Q1860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>301</td>\n",
       "      <td>61</td>\n",
       "      <td>9QT</td>\n",
       "      <td>Is this triple correct or incorrect?</td>\n",
       "      <td>$0.50</td>\n",
       "      <td>302</td>\n",
       "      <td>HHCKW1111</td>\n",
       "      <td>Submitted</td>\n",
       "      <td>200</td>\n",
       "      <td>80%</td>\n",
       "      <td>wd:Q1288004</td>\n",
       "      <td>wdt:P1412</td>\n",
       "      <td>wd:Q13330</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CORRECT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>302</td>\n",
       "      <td>61</td>\n",
       "      <td>9QT</td>\n",
       "      <td>Is this triple correct or incorrect?</td>\n",
       "      <td>$0.50</td>\n",
       "      <td>303</td>\n",
       "      <td>GGUI83657S</td>\n",
       "      <td>Submitted</td>\n",
       "      <td>120</td>\n",
       "      <td>85%</td>\n",
       "      <td>wd:Q1288004</td>\n",
       "      <td>wdt:P1412</td>\n",
       "      <td>wd:Q13330</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CORRECT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>224 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  HITId HITTypeId                                 Title Reward  \\\n",
       "0             0      1       7QT  Is this triple correct or incorrect?  $0.50   \n",
       "1             1      1       7QT  Is this triple correct or incorrect?  $0.50   \n",
       "2             2      1       7QT  Is this triple correct or incorrect?  $0.50   \n",
       "3             3      1       7QT  Is this triple correct or incorrect?  $0.50   \n",
       "4             6      2       7QT  Is this triple correct or incorrect?  $0.50   \n",
       "..          ...    ...       ...                                   ...    ...   \n",
       "219         296     60       9QT  Is this triple correct or incorrect?  $0.50   \n",
       "220         297     60       9QT  Is this triple correct or incorrect?  $0.50   \n",
       "221         300     61       9QT  Is this triple correct or incorrect?  $0.50   \n",
       "222         301     61       9QT  Is this triple correct or incorrect?  $0.50   \n",
       "223         302     61       9QT  Is this triple correct or incorrect?  $0.50   \n",
       "\n",
       "     AssignmentId     WorkerId AssignmentStatus  WorkTimeInSeconds  \\\n",
       "0               1  2133ICYWE97        Submitted                 60   \n",
       "1               2  2133U7HKDLO        Submitted                 40   \n",
       "2               3  928UJANWZ12        Submitted                 50   \n",
       "3               4    1726JMZQW        Submitted                 80   \n",
       "4               7  2133ICYWE97        Submitted                120   \n",
       "..            ...          ...              ...                ...   \n",
       "219           297    HHCKW1111        Submitted                200   \n",
       "220           298   GGUI83657S        Submitted                120   \n",
       "221           301    AALKMII97        Submitted                240   \n",
       "222           302    HHCKW1111        Submitted                200   \n",
       "223           303   GGUI83657S        Submitted                120   \n",
       "\n",
       "    LifetimeApprovalRate      Input1ID   Input2ID   Input3ID  AnswerID  \\\n",
       "0                    99%     wd:Q11621  wdt:P2142  792910554       1.0   \n",
       "1                    40%     wd:Q11621  wdt:P2142  792910554       1.0   \n",
       "2                    98%     wd:Q11621  wdt:P2142  792910554       2.0   \n",
       "3                    70%     wd:Q11621  wdt:P2142  792910554       1.0   \n",
       "4                    99%    wd:Q603545  wdt:P2142    4300000       1.0   \n",
       "..                   ...           ...        ...        ...       ...   \n",
       "219                  80%  wd:Q21060270    wdt:P27    wd:Q916       1.0   \n",
       "220                  85%  wd:Q21060270    wdt:P27    wd:Q916       1.0   \n",
       "221                  98%   wd:Q1288004  wdt:P1412  wd:Q13330       2.0   \n",
       "222                  80%   wd:Q1288004  wdt:P1412  wd:Q13330       1.0   \n",
       "223                  85%   wd:Q1288004  wdt:P1412  wd:Q13330       1.0   \n",
       "\n",
       "    AnswerLabel FixPosition FixValue  \n",
       "0       CORRECT         NaN      NaN  \n",
       "1       CORRECT         yes      yes  \n",
       "2     INCORRECT         NaN      NaN  \n",
       "3       CORRECT         NaN      NaN  \n",
       "4       CORRECT         NaN      NaN  \n",
       "..          ...         ...      ...  \n",
       "219     CORRECT         NaN      NaN  \n",
       "220     CORRECT         NaN      NaN  \n",
       "221   INCORRECT      Object    Q1860  \n",
       "222     CORRECT         NaN      NaN  \n",
       "223     CORRECT         NaN      NaN  \n",
       "\n",
       "[224 rows x 17 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_crowd_pd = pd.read_csv('Data/crowd_data/clean_crowd_data.csv')\n",
    "clean_crowd_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "404e1218",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data/crowd_data/rates.json\", \"r\") as f:\n",
    "    rates = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bdd860",
   "metadata": {},
   "source": [
    "# Loading Graph and other files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "045fb91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data/images.json\", \"r\") as f:\n",
    "    images = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3de3a25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "category2URIID = {\n",
    "            'PER': {'ids':['Q33999', 'Q10800557', 'Q2526255'], 'cat': 'P106'},\n",
    "            'MISC': {'ids':['Q11424', 'Q20650540', 'Q29168811', 'Q24862', 'Q24865', 'Q24869'], 'cat': 'P31'},\n",
    "\n",
    "        }\n",
    "film_properties = set(pd.read_csv('Data/Film Properties.csv')['res'])\n",
    "\n",
    "noun_mapper = {\n",
    "        'cast member' : set(['actor', 'actress']),\n",
    "        'genre': set(['type', 'kind']),\n",
    "        'publication date': set(['release', 'date', 'airdate', 'publication', 'launch', 'broadcast']),\n",
    "        'executive producer': set(['showrunner']),\n",
    "        'screenwriter': set(['scriptwriter', 'screenplay', 'teleplay', 'writer', 'script', 'scenarist', 'story']),\n",
    "        'director of photography': set(['cinematographer', 'DOP', 'dop']),\n",
    "        'film editor': set(['editor']),\n",
    "        'production designer': set(['designer']),\n",
    "        'cost': set(['budget']),\n",
    "        'box office': set(['box', 'earning']),\n",
    "        'nominated for': set(['nomination', 'award', 'finalist', 'shortlist', 'selection']),\n",
    "        'official website' : set(['website', 'site']),\n",
    "        'filming location' : set(['flocation']),\n",
    "        'narrative website' : set(['nlocation']),\n",
    "        'production company' : set(['company']),\n",
    "        'country of origin': set(['origin', 'country'])\n",
    "    \n",
    "\n",
    "        }\n",
    "\n",
    "\n",
    "noun_film_properties = set()\n",
    "for v in noun_mapper.values():\n",
    "    noun_film_properties.update(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "759e81ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Box Office Mojo film ID (former scheme)',\n",
       " 'FilmAffinity ID',\n",
       " 'Filmportal ID',\n",
       " 'IMDb ID',\n",
       " 'Kinopoisk film ID',\n",
       " 'average shot length',\n",
       " 'award received',\n",
       " 'based on',\n",
       " 'box office',\n",
       " 'cast member',\n",
       " 'composer',\n",
       " 'cost',\n",
       " 'costume designer',\n",
       " 'country of origin',\n",
       " 'director',\n",
       " 'director of photography',\n",
       " 'distributed by',\n",
       " 'duration',\n",
       " 'executive producer',\n",
       " 'exploitation visa number',\n",
       " 'film editor',\n",
       " 'film script',\n",
       " 'filming location',\n",
       " 'genre',\n",
       " 'lyrics by',\n",
       " 'main subject',\n",
       " 'media franchise',\n",
       " 'narrative location',\n",
       " 'original film format',\n",
       " 'original language of film or TV show',\n",
       " 'part of the series',\n",
       " 'producer',\n",
       " 'production designer',\n",
       " 'publication date',\n",
       " 'screenwriter',\n",
       " 'set in period',\n",
       " 'title'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "film_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5291983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DOP',\n",
       " 'actor',\n",
       " 'actress',\n",
       " 'airdate',\n",
       " 'award',\n",
       " 'box',\n",
       " 'broadcast',\n",
       " 'budget',\n",
       " 'cinematographer',\n",
       " 'company',\n",
       " 'country',\n",
       " 'date',\n",
       " 'designer',\n",
       " 'dop',\n",
       " 'earning',\n",
       " 'editor',\n",
       " 'finalist',\n",
       " 'flocation',\n",
       " 'kind',\n",
       " 'launch',\n",
       " 'nlocation',\n",
       " 'nomination',\n",
       " 'origin',\n",
       " 'publication',\n",
       " 'release',\n",
       " 'scenarist',\n",
       " 'screenplay',\n",
       " 'script',\n",
       " 'scriptwriter',\n",
       " 'selection',\n",
       " 'shortlist',\n",
       " 'showrunner',\n",
       " 'site',\n",
       " 'story',\n",
       " 'teleplay',\n",
       " 'type',\n",
       " 'website',\n",
       " 'writer'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_film_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8e8460b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "\n",
    "from pos_extraction import *\n",
    "from ner_extraction import *\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "graph = rdflib.Graph().parse('Data/14_graph.nt', format='turtle')\n",
    "\n",
    "WDT = rdflib.Namespace('http://www.wikidata.org/prop/direct/')\n",
    "WD = rdflib.Namespace('http://www.wikidata.org/entity/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f40ab93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NER models...\n",
      "2022-12-14 11:30:08,653 loading file models/ner_large\n",
      "2022-12-14 11:30:43,333 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>\n",
      "2022-12-14 11:30:45,365 loading file models/ner_base\n",
      "2022-12-14 11:30:47,792 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>\n",
      "Loading Entity name similarity model...\n",
      "Loading Title embeddings\n",
      "Loading POS model...\n",
      "2022-12-14 11:30:51,884 loading file models/pos1\n",
      "2022-12-14 11:30:53,346 SequenceTagger predicts: Dictionary with 53 tags: <unk>, O, UH, ,, VBD, PRP, VB, PRP$, NN, RB, ., DT, JJ, VBP, VBG, IN, CD, NNS, NNP, WRB, VBZ, WDT, CC, TO, MD, VBN, WP, :, RP, EX, JJR, FW, XX, HYPH, POS, RBR, JJS, PDT, NNPS, RBS, AFX, WP$, -LRB-, -RRB-, ``, '', LS, $, SYM, ADD\n"
     ]
    }
   ],
   "source": [
    "ner_extractor = NER_extractor()\n",
    "pos_extractor = POS_extractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfff816e",
   "metadata": {},
   "source": [
    "# Create entity name text embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edd411d",
   "metadata": {},
   "source": [
    "The next cells goal is to create title embeddings to match spell mistakes on entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2940e82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# with open('Data/ddis-graph-embeddings/entity_ids.del', 'r') as ifile:\n",
    "#     ent_list = [ent.split('/')[-1] for idx, ent in csv.reader(ifile, delimiter='\\t')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "edf70252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1015de9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ent2name = {}\n",
    "# tt = tqdm(ent_list)\n",
    "# for ent in tt:\n",
    "#     try:\n",
    "#         q = f'''        \n",
    "#         prefix wdt: <http://www.wikidata.org/prop/direct/>\n",
    "#         prefix wd: <http://www.wikidata.org/entity/>\n",
    "\n",
    "#         SELECT ?lbl \n",
    "#         WHERE {{\n",
    "#           wd:{ent} rdfs:label ?lbl .\n",
    "#           FILTER(LANG(?lbl) = \"en\").\n",
    "#           }}\n",
    "#           LIMIT 1'''\n",
    "#         res = str(list(graph.query(q))[0][0])\n",
    "\n",
    "#         ent2name[ent] = res\n",
    "#     except:\n",
    "#         continue\n",
    "# print(len(ent2name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b61c94d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ent2emb = {}\n",
    "# ttt = tqdm(ent2name.items())\n",
    "# for k,v in ttt:\n",
    "#     emb = model.encode(v).tolist()\n",
    "#     ent2emb[k] = emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "241568eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# em_values = np.array(list(ent2emb.values()))\n",
    "# em_keys = list(ent2emb.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72449132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('Data/title_embeddings.npy', em_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c74be938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load json module\n",
    "# import json\n",
    "\n",
    "# # create json object from dictionary\n",
    "# j = json.dumps(em_keys)\n",
    "\n",
    "# # open file for writing, \"w\" \n",
    "# f = open(\"Data/titles.json\",\"w\")\n",
    "\n",
    "# # write json object to file\n",
    "# f.write(j)\n",
    "\n",
    "# # close file\n",
    "# f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10672684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load json module\n",
    "# import json\n",
    "\n",
    "# # create json object from dictionary\n",
    "# j = json.dumps(ent2name)\n",
    "\n",
    "# # open file for writing, \"w\" \n",
    "# f = open(\"Data/ent2name.json\",\"w\")\n",
    "\n",
    "# # write json object to file\n",
    "# f.write(j)\n",
    "\n",
    "# # close file\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f44dbec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name2ent = {}\n",
    "# for k,v in ent2name.items():\n",
    "#     if name2ent.get(v):\n",
    "#         name2ent[v].append(k)\n",
    "#     else:\n",
    "#         name2ent[v] = [k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b245926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create json object from dictionary\n",
    "# j = json.dumps(name2ent)\n",
    "\n",
    "# # open file for writing, \"w\" \n",
    "# f = open(\"Data/name2ent.json\",\"w\")\n",
    "\n",
    "# # write json object to file\n",
    "# f.write(j)\n",
    "\n",
    "# # close file\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f7ce88",
   "metadata": {},
   "source": [
    "# Modular Code parts (some older versions than in Agent's python files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "41b888c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def _EntityURI_to_ID(URI_LIST, WD='http://www.wikidata.org/entity/'):\n",
    "    res = []\n",
    "    for uri in URI_LIST:\n",
    "\n",
    "        if WD in uri:\n",
    "            res.append(re.match(\"{}(.*)\".format(WD), uri)[1])\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def _getEntity_URI_ID(graph, ent, WDT, WD, cat2id):\n",
    "    query = f'''\n",
    "        prefix wdt: <http://www.wikidata.org/prop/direct/>\n",
    "        prefix wd: <http://www.wikidata.org/entity/>\n",
    "\n",
    "        SELECT ?res\n",
    "        WHERE{{\n",
    "            ?res rdfs:label \"{ent}\"@en.\n",
    "            }}'''\n",
    "    URI_LIST = [str(x[0]) for x in list(graph.query(query))]\n",
    "\n",
    "    entities_ids = _EntityURI_to_ID( URI_LIST, WD)\n",
    "\n",
    "    res = {}\n",
    "    for e_id in entities_ids:\n",
    "        for k, v in cat2id.items():\n",
    "            g = list(graph.objects(WD[e_id], WDT[v['cat']]))\n",
    "        \n",
    "            instancesOf = _EntityURI_to_ID(g, WD)\n",
    "            if instancesOf and set(v['ids']).intersection(instancesOf):\n",
    "                if res.get(k):\n",
    "                    res[k].append({'entity':ent, 'id':e_id})\n",
    "                else:\n",
    "                    res[k] = [{'entity':ent, 'id':e_id}]\n",
    "\n",
    "\n",
    "    return res\n",
    "\n",
    "def getEntities_URIIDs(graph, entities, WDT, WD, cat2id):\n",
    "    qres = []\n",
    "    for e in entities:\n",
    "        uri_res = _getEntity_URI_ID(graph, e, WDT, WD, cat2id)\n",
    "        if not uri_res and re.search('-', e):\n",
    "            uri_res = _getEntity_URI_ID(graph, re.sub('-', '–', e), WDT, WD, cat2id)\n",
    "        elif not uri_res and re.search('–', e):\n",
    "            uri_res = _getEntity_URI_ID(graph, re.sub('–', '-', e), WDT, WD, cat2id)\n",
    "        qres.append(uri_res)\n",
    "    \n",
    "    entities_uriID = {}\n",
    "    for q in qres:\n",
    "        for k, v in q.items():\n",
    "            if entities_uriID.get(k):\n",
    "                entities_uriID[k].extend(v)\n",
    "            else:\n",
    "                entities_uriID[k] = v\n",
    "                \n",
    "    return entities_uriID\n",
    "\n",
    "def _get_model_res(model, text):\n",
    "    sentence = Sentence(testsent)\n",
    "    model.predict(sentence)\n",
    "    \n",
    "    Owords = []\n",
    "    ent_words = []\n",
    "    idx = []\n",
    "    ent = []\n",
    "    for entity in sentence.get_spans('ner'):\n",
    "        if entity.get_labels('ner')[0].value in ['PER', 'MISC']:\n",
    "            ent.append(entity.get_labels('ner')[0].value)\n",
    "            ent_words.append(testsent[entity.start_position:entity.end_position])\n",
    "            idx.append((entity.start_position,entity.end_position))\n",
    "\n",
    "        \n",
    "    if len(idx) == 1:\n",
    "        Owords.append(text[0:idx[0][0]])\n",
    "        Owords.append(text[idx[0][1]:])\n",
    "    else:\n",
    "        for i, v in enumerate(idx):\n",
    "\n",
    "            if i == 0:\n",
    "                Owords.append(text[0:v[0]])\n",
    "                continue\n",
    "            Owords.append(text[idx[i-1][1]:v[0]])    \n",
    "            if i == len(idx)-1:\n",
    "                Owords.append(text[v[1]:])\n",
    "        \n",
    "    return  ent_words, ent, Owords\n",
    "\n",
    "def get_entities(text):\n",
    "    if text[-1] == '?' or text[-1] == '.':\n",
    "        text = text[:-1]\n",
    "    \n",
    "    \n",
    "    ent_words1, ent1, Owords1 = _get_model_res(ner_large, text)\n",
    "    ent_words2, ent2, Owords2 = _get_model_res(ner_base, text)\n",
    "    \n",
    "    print([ent_words1, ent1, Owords1])\n",
    "    print([ent_words2, ent2, Owords2])\n",
    "    \n",
    "    \n",
    "    if len(ent_words1) > len(ent_words2) and ent_words2:\n",
    "        word_group = ent_words2\n",
    "        entities = ent2\n",
    "        Owords = Owords2\n",
    "    else:\n",
    "        word_group = ent_words1\n",
    "        entities = ent1\n",
    "        Owords = Owords1\n",
    "\n",
    "\n",
    "    print()\n",
    "    print('NER')\n",
    "    print(Owords)\n",
    "    print(word_group)\n",
    "    print(entities)\n",
    "    print()\n",
    "\n",
    "    return  word_group, Owords if entities else [text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "25f944c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(text):\n",
    "\n",
    "    sentence = Sentence(text)\n",
    "\n",
    "    tagger.predict(sentence)\n",
    "\n",
    "    pos_words = []\n",
    "    pos_tags = []\n",
    "    for entity in sentence:\n",
    "        pos_words.append(entity.text)\n",
    "        pos_tags.append(entity.get_labels('pos')[0].value)\n",
    "\n",
    "    print('POS')\n",
    "    print(pos_words)\n",
    "    print(pos_tags)\n",
    "    print()\n",
    "    pop = list(zip(pos_words, pos_tags))\n",
    "\n",
    "    return pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3681abf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_pos(testsent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "295500b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "\n",
    "def _pos_to_word_index(pos_tags, Osent):\n",
    "    res = {}\n",
    "    for i in pos_tags:\n",
    "        w = i[0]\n",
    "        p = i[1]\n",
    "        try:\n",
    "            if re.search(w.lower(), Osent.lower()):\n",
    "                if res.get(p):\n",
    "                    res[p].append(w)\n",
    "                else:\n",
    "                    res[p] = [w]\n",
    "        except:\n",
    "            continue\n",
    "    return res\n",
    "\n",
    "def get_relations(pos_tags, Owords, graph, WDT, film_properties):\n",
    "\n",
    "    properties = set()\n",
    "    properties.update(noun_film_properties)\n",
    "    properties.update(film_properties)\n",
    "    \n",
    "    Osent = ' '.join(Owords)\n",
    "\n",
    "    pos_text_dict = _pos_to_word_index(pos_tags, Osent)\n",
    "    print(pos_text_dict)\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    fil = [\n",
    "        re.search('where', Osent.lower()) and re.search('film', Osent.lower()),\n",
    "        re.search('location', Osent.lower()) and re.search('film', Osent.lower()),\n",
    "        re.search('place', Osent.lower()) and  re.search('film', Osent.lower()),\n",
    "        re.search('shooting', Osent.lower()) and re.search('location', Osent.lower()),\n",
    "        re.search('shot in', Osent.lower()),\n",
    "        re.search('filmed in', Osent.lower())\n",
    "    ]\n",
    "    \n",
    "    if any(fil):\n",
    "        res.append({'relation':'filming location', 'ids': ['P915']})\n",
    "        \n",
    "    narr = [\n",
    "        re.search('where', Osent.lower()) and re.search('narrat', Osent.lower()),\n",
    "        re.search('where', Osent.lower()) and re.search('set', Osent.lower()),\n",
    "        re.search('where', Osent.lower()) and re.search('takes place', Osent.lower()),\n",
    "        re.search('place', Osent.lower()) and re.search('set', Osent.lower()),\n",
    "        re.search('location', Osent.lower()) and re.search('set', Osent.lower()),\n",
    "        re.search('location', Osent.lower()) and re.search('narrat', Osent.lower()),\n",
    "        re.search('set', Osent.lower()) and re.search('work', Osent.lower()),\n",
    "        \n",
    "    ]\n",
    "    if any(narr):\n",
    "        res.append({'relation':'narrative location', 'ids': ['P840']})\n",
    "    \n",
    "    if re.search('MPA', Osent):\n",
    "        res.append({'relation':'MPAA rating', 'ids': ['P1657']})\n",
    "        \n",
    "    if re.search('FSK', Osent):\n",
    "        res.append({'relation': 'FSK film rating', 'ids': ['P1981']})\n",
    "\n",
    "        \n",
    "    picture = [re.search('look like', Osent.lower()),\n",
    "               re.search('looks like', Osent.lower()),\n",
    "               re.search('picture', Osent.lower()),\n",
    "               re.search('poster', Osent.lower())]\n",
    "        \n",
    "    if any(picture):\n",
    "        res.append({'relation': 'IMDb ID', 'ids': ['P345']})\n",
    "        \n",
    "    recom = [re.search('recommend', Osent.lower()),\n",
    "               re.search('recommendation', Osent.lower()),\n",
    "               re.search('suggest', Osent.lower()),\n",
    "               re.search('suggestion', Osent.lower())]\n",
    "        \n",
    "    if any(recom):\n",
    "        res.append({'relation': 'recommendation', 'ids': []})\n",
    "\n",
    "    nouns = []\n",
    "    for pos in pos_text_dict.keys():\n",
    "\n",
    "        if pos[:2] == 'VB':\n",
    "            for w in pos_text_dict[pos]:\n",
    "                noun_conversions = get_word_forms(w)['n']\n",
    "                matching_conversions = set(properties).intersection(noun_conversions)\n",
    "                for noun in matching_conversions:\n",
    "                    if noun in film_properties:\n",
    "                        nouns.append(noun)\n",
    "                    elif noun in noun_film_properties:\n",
    "                        for k, v in noun_mapper.items():\n",
    "                            if noun in v:\n",
    "                                nouns.append(k)\n",
    "                                break\n",
    "\n",
    "        elif pos == 'NNS':\n",
    "            for w in pos_text_dict[pos]:\n",
    "                noun = inflect_engine.singular_noun(w)\n",
    "                if noun in film_properties:\n",
    "                    nouns.append(noun)\n",
    "                elif noun in noun_film_properties:\n",
    "                    for k, v in noun_mapper.items():\n",
    "                        if noun in v:\n",
    "                            nouns.append(k)\n",
    "                            break\n",
    "                \n",
    "    print(pos_text_dict.get('NN', []))\n",
    "    for noun in pos_text_dict.get('NN', []):\n",
    "        if noun in film_properties:\n",
    "            nouns.append(noun)\n",
    "        elif noun in noun_film_properties:\n",
    "            for k, v in noun_mapper.items():\n",
    "                if noun in v:\n",
    "                    nouns.append(k)\n",
    "                    break\n",
    "\n",
    "    for noun in set(nouns):\n",
    "        res.append({'relation': noun, 'ids': pos_extractor._getRelation_URI_ID(graph, noun, WDT)})\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b0789cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = f'''prefix wdt: <http://www.wikidata.org/prop/direct/>\n",
    "# prefix wd: <http://www.wikidata.org/entity/>\n",
    "\n",
    "# SELECT ?res\n",
    "# WHERE{{\n",
    "#   ?res rdfs:label \"costume designer\"@en.\n",
    "# }}'''\n",
    "# list(graph.query(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f6661e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_extractor._getRelation_URI_ID(graph, 'budget', WDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb3ed109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _Relation_URI_ID(uris, WDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "539289d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n': {'directive',\n",
       "  'directiveness',\n",
       "  'directivenesses',\n",
       "  'directives',\n",
       "  'directivities',\n",
       "  'directivity',\n",
       "  'directness',\n",
       "  'directnesses',\n",
       "  'director',\n",
       "  'directors',\n",
       "  'directorship',\n",
       "  'directorships'},\n",
       " 'a': {'direct', 'directive'},\n",
       " 'v': {'direct', 'directed', 'directing', 'directs'},\n",
       " 'r': {'direct', 'directly'}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_forms('Director'.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c1330e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings(entity_emb, ent2id, relation_emb, rel2id, ent, rel, num_ret):\n",
    "    professions = set(graph.query(f'''\n",
    "    prefix wdt: <http://www.wikidata.org/prop/direct/>\n",
    "    prefix wd: <http://www.wikidata.org/entity/>\n",
    "    \n",
    "    SELECT ?obj ?lbl WHERE {{\n",
    "        wd:{ent} wdt:{rel} ?obj .\n",
    "        ?obj rdfs:label ?lbl .\n",
    "    }}\n",
    "    '''))\n",
    "    try:\n",
    "        \n",
    "        # \"Jean Van Hamme\" entity\n",
    "        head = entity_emb[ent2id[WD[ent]]]\n",
    "        # \"occupation\" relation\n",
    "        pred = relation_emb[rel2id[WDT[rel]]]\n",
    "        # add vectors according to TransE scoring function.\n",
    "        lhs = head + pred\n",
    "        # compute distance to *any* entity\n",
    "        dist = pairwise_distances(lhs.reshape(1, -1), entity_emb).reshape(-1)\n",
    "        # find most plausible entities\n",
    "        most_likely = dist.argsort()\n",
    "\n",
    "\n",
    "        return [{'label':ent2lbl[id2ent[idx]], 'Score': dist[idx]} for idx in most_likely[:num_ret]]\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "#     return pd.DataFrame([(id2ent[idx][len(WD):], ent2lbl[id2ent[idx]], dist[idx], rank+1)\n",
    "#                          for rank, idx in enumerate(most_likely[:10])],\n",
    "#                         columns=('Entity', 'Label', 'Score', 'Rank'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6ab0306c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(graph.objects(WD['Q193835'], WDT['P1411']))\n",
    "# plots_embeddings\n",
    "def movie_recom_movie(ent, WD, entity_emb, ent2id, id2ent, ent2lbl):\n",
    "    m_ids = [p['id'] for p in ent['MISC']]\n",
    "    \n",
    "    mean_emb = np.mean([entity_emb[ent2id[WD[idd]]] for idd in m_ids], 0)\n",
    "\n",
    "    dist = pairwise_distances(mean_emb.reshape(1, -1), entity_emb).reshape(-1)\n",
    "    # find most plausible entities\n",
    "    most_likely = dist.argsort()\n",
    "    \n",
    "    res = []\n",
    "    for idx in most_likely[:15+len(m_ids)]:\n",
    "        g = list(filter(lambda x: str(x).split('/')[-1] in category2URIID['MISC']['ids'], \n",
    "                        list(graph.objects(id2ent[idx], WDT.P31))))\n",
    "\n",
    "        if g and id2ent[idx][len(WD):] not in m_ids and ent2lbl.get(id2ent[idx]):\n",
    "            res.append({'ent':id2ent[idx][len(WD):], 'label':ent2lbl[id2ent[idx]], 'Score': dist[idx]})\n",
    "            \n",
    "    return res[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b3564d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def movie_recom_genre(graph, genre):\n",
    "    query = f'''\n",
    "    prefix wdt: <http://www.wikidata.org/prop/direct/>\n",
    "    prefix wd: <http://www.wikidata.org/entity/>\n",
    "\n",
    "    SELECT ?obj ?lbl \n",
    "    WHERE {{\n",
    "      ?obj wdt:P136 wd:{genre} .\n",
    "      ?obj rdfs:label ?lbl .\n",
    "      FILTER(LANG(?lbl) = \"en\").\n",
    "      }}\n",
    "    ORDER BY RAND() LIMIT 1\n",
    "    \n",
    "    '''\n",
    "    qres = list(graph.query(query))[0]\n",
    "\n",
    "    res = {'ent':str(qres[0]).split('/')[-1], 'label': str(qres[1])}\n",
    "\n",
    "    return res\n",
    "# movie_recom_genre(graph, 'Q130232')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c4061c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ent': 'Q1576873', 'label': 'Fast & Furious'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def movie_recom_actor_genre(graph, actor, genre):\n",
    "    query = f'''\n",
    "    prefix wdt: <http://www.wikidata.org/prop/direct/>\n",
    "    prefix wd: <http://www.wikidata.org/entity/>\n",
    "\n",
    "    SELECT ?obj ?lbl \n",
    "    WHERE {{\n",
    "      ?obj wdt:P136 wd:{genre}  .\n",
    "      ?obj wdt:P161 wd:{actor} .\n",
    "      ?obj rdfs:label ?lbl .\n",
    "      FILTER(LANG(?lbl) = \"en\").\n",
    "    }}\n",
    "    ORDER BY RAND() LIMIT 1\n",
    "    \n",
    "    '''\n",
    "    qres = list(graph.query(query))[0]\n",
    "\n",
    "    res = {'ent':str(qres[0]).split('/')[-1], 'label': str(qres[1])}\n",
    "\n",
    "    return res\n",
    "movie_recom_actor_genre(graph, 'Q169963', 'Q188473')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fccb48ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def movie_recom_actor(graph, actor):\n",
    "    query = f'''\n",
    "    prefix wdt: <http://www.wikidata.org/prop/direct/>\n",
    "    prefix wd: <http://www.wikidata.org/entity/>\n",
    "\n",
    "    SELECT ?obj ?lbl \n",
    "    WHERE {{\n",
    "      ?obj wdt:P161 wd:{actor} .\n",
    "      ?obj rdfs:label ?lbl .\n",
    "      FILTER(LANG(?lbl) = \"en\").\n",
    "    }}\n",
    "    ORDER BY RAND() LIMIT 1\n",
    "    \n",
    "    '''\n",
    "    qres = list(graph.query(query))[0]\n",
    "\n",
    "    res = {'ent':str(qres[0]).split('/')[-1], 'label': str(qres[1])}\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "af07a198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # recommendation\n",
    "# final_ans = ''\n",
    "# if 'recommendation' in [r['relation'] for r in rel]:\n",
    "#     if ent.get('MISC'):\n",
    "#         res = movie_recom_movie(ent, WD, entity_emb, ent2id, id2ent, ent2lbl)\n",
    "#         ent_print_list = []\n",
    "#         for r in res:\n",
    "#             ent_print_list.append(f\"{r['label']}{get_movie_date(graph, r['ent'])}\")\n",
    "#         final_ans += f\" Hmm... I could recommend you {', '.join(ent_print_list)}.\"\n",
    "#     else: \n",
    "#         Osent = ' '.join(Owords)\n",
    "#         genre = ''\n",
    "#         for v in genre_dict.values():\n",
    "#             words = v['words']\n",
    "#             found = False\n",
    "#             for w in words:\n",
    "#                 if re.search(w, Osent.lower()):\n",
    "#                     genre = v['id']\n",
    "#                     found = True\n",
    "#                     break\n",
    "#             if found:\n",
    "#                 break\n",
    "                    \n",
    "#         if ent.get('PER') and genre:\n",
    "#             for actor in ent['PER']:\n",
    "#                 res = movie_recom_actor_genre(graph, actor['id'], genre)\n",
    "#                 ent_print = f\"{res['label']}{get_movie_date(graph, res['ent'])}\"\n",
    "#                 final_ans += f\" Hmm... I could recommend you {ent_print}.\"\n",
    "#         elif ent.get('PER'):\n",
    "#             for actor in ent['PER']:\n",
    "#                 res = movie_recom_actor(graph, actor['id'])\n",
    "#                 ent_print = f\"{res['label']}{get_movie_date(graph, res['ent'])}\"\n",
    "#                 final_ans += f\" Hmm... I could recommend you {ent_print}.\"\n",
    "#         elif genre:\n",
    "#             res = movie_recom_genre(graph, genre)\n",
    "#             ent_print = f\"{res['label']}{get_movie_date(graph, res['ent'])}\"\n",
    "#             final_ans += f\" Hmm... I could recommend you {ent_print}.\"\n",
    "    \n",
    "    \n",
    "# final_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3d356c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' (2010)'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_movie_date(graph, ent):\n",
    "    query = f'''\n",
    "        prefix wdt: <http://www.wikidata.org/prop/direct/>\n",
    "        prefix wd: <http://www.wikidata.org/entity/>\n",
    "\n",
    "        SELECT ?obj \n",
    "        WHERE {{\n",
    "          wd:{ent} wdt:P577  ?obj.\n",
    "        }} ORDER BY ASC(?obj)\n",
    "         LIMIT 1\n",
    "    \n",
    "    '''\n",
    "    return f\" ({str(max(list(graph.query(query))[0][0].split('-')))})\" if list(graph.query(query)) else ''\n",
    "get_movie_date(graph,'Q1410031')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e0e315e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crowdsource_search(ent, rel):\n",
    "    res = clean_crowd_pd[(clean_crowd_pd['Input1ID'] == f\"wd:{ent}\") & (clean_crowd_pd['Input2ID'] == f\"wdt:{rel}\")]\n",
    "\n",
    "    if bool(list(res.HITId)):\n",
    "        agg_dict = {}\n",
    "        for k,v in list(zip(res.Input3ID, res.AnswerLabel)):\n",
    "            if agg_dict.get(k):\n",
    "                agg_dict[k].append(v)\n",
    "            else:\n",
    "                agg_dict[k] = [v]\n",
    "                \n",
    "        agg_res = {}\n",
    "        for k, v in agg_dict.items():\n",
    "            agg_res[k] = max(set(v), key=v.count)\n",
    "\n",
    "        hitid = list(set(res.HITId))[0]\n",
    "        rate_ans = rates[str(hitid)]\n",
    "        ans =  str(list(agg_res.keys())[0]).strip('wd:')\n",
    "        state = list(agg_res.values())[0]\n",
    "        \n",
    "        filter_fix_val = set(filter(lambda x: isinstance(x, str), list(res.FixValue)))\n",
    "        \n",
    "        if list(agg_res.values())[0] == 'INCORRECT' and filter_fix_val:\n",
    "            ans = str(list(filter_fix_val)[0]).strip('wd:')\n",
    "            state = 'CORRECT'\n",
    "                       \n",
    "        return ans, rate_ans, state\n",
    "    else:\n",
    "        return None, None, None\n",
    "\n",
    "# crowdsource_search('Q171300','P2142')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d6571e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # retrieval\n",
    "# retrieval = {}\n",
    "# final_ans = ''\n",
    "\n",
    "# for k, v in ent.items():\n",
    "\n",
    "#     for ee in v:\n",
    "#         retrieval[ee['entity']] = {}\n",
    "#         for relation in rel:\n",
    "#             retrieval[ee['entity']][relation['relation']] = []\n",
    "#             for rid in relation['ids']:\n",
    "                \n",
    "#                 g = list(graph.objects(WD[ee['id']], WDT[rid]))\n",
    "                \n",
    "#                 if relation['relation'] == 'IMDb ID':\n",
    "#                     imbds = []\n",
    "#                     for i in g:\n",
    "#                         imbds.append(str(i))\n",
    "#                     imdb_id = imbds[0]\n",
    "\n",
    "#                     im_id = ''\n",
    "#                     for image in images:\n",
    "#                         if k == 'MISC':\n",
    "#                             if image['movie'] == [imdb_id] and image['type'] == 'poster':\n",
    "#                                 im_id = image['img']\n",
    "#                                 im_id = 'image:'+im_id.strip('.jpg')\n",
    "#                                 break\n",
    "#                         else:\n",
    "#                             if image['cast'] == [imdb_id] and image['type'] != 'poster':\n",
    "#                                 im_id = image['img']\n",
    "#                                 im_id = 'image:'+im_id.strip('.jpg')\n",
    "#                                 break\n",
    "\n",
    "\n",
    "#                     if im_id:\n",
    "#                         final_ans += f\"There you go... {im_id}\"\n",
    "                    \n",
    "#                 # knowledge graph on these particular relations    \n",
    "#                 elif relation['relation'] in ['publication date', 'cost', 'box office']:\n",
    "#                     kg_res = str(g[0]) if g else None\n",
    "                    \n",
    "#                     cs_ans, cs_rate, cs_state = crowdsource_search(ee['id'], rid)\n",
    "                    \n",
    "#                     if kg_res and cs_ans:\n",
    "#                         if kg_res == cs_ans:\n",
    "#                             if cs_state == 'CORRECT' and cs_rate > 0.5:\n",
    "#                                 final_ans += f\" The {relation['relation']} of {ee['entity']} is {kg_res} (Crowd Approval rate: {cs_rate}).\"\n",
    "#                             else:\n",
    "#                                 final_ans += f\" I think the {relation['relation']} of {ee['entity']} is {kg_res} but im not really sure (Crowd Approval rate: {cs_rate}).\"\n",
    "#                         else:\n",
    "#                             if cs_state == 'CORRECT' and cs_rate > 0.5:\n",
    "#                                 final_ans += f\" The {relation['relation']} of {ee['entity']} is {cs_ans} (Crowd Approval rate: {cs_rate}).\"\n",
    "#                             else:\n",
    "#                                 final_ans += f\" I think the {relation['relation']} of {ee['entity']} is {kg_res} but im not really sure (Crowd Approval rate: {cs_rate}).\"\n",
    "#                     elif cs_ans:\n",
    "#                         if cs_state == 'CORRECT' and cs_rate > 0.5:\n",
    "#                             final_ans += f\" The {relation['relation']} of {ee['entity']} is {cs_ans} (Crowd Approval rate: {cs_rate}).\"\n",
    "#                         else:\n",
    "#                             final_ans += f\" I don't really know the {relation['relation']} of {ee['entity']}. But {cs_ans} is not (Crowd Approval rate: {cs_rate}).\"\n",
    "#                     elif kg_res:\n",
    "#                         final_ans += f\" The {relation['relation']} of {ee['entity']} is {kg_res}.\"\n",
    "                                \n",
    "                    \n",
    "#                 else: #knowledge graph\n",
    "#                     ent_print = f\"{ee['entity']}{get_movie_date(graph, ee['id'])}\"\n",
    "#                     print(g)\n",
    "#                     result = _EntityURI_to_ID(g)\n",
    "                    \n",
    "#                     # crowd source for 1 result or no result on KG\n",
    "#                     cs_ans, cs_rate, cs_state = None, None, None\n",
    "#                     if len(result) < 2:\n",
    "#                         cs_ans, cs_rate, cs_state = crowdsource_search(ee['id'], rid)\n",
    "                        \n",
    "                        \n",
    "#                     kg_res = []\n",
    "#                     for r in result:\n",
    "\n",
    "#                         query = f'''\n",
    "#                                 prefix wdt: <http://www.wikidata.org/prop/direct/>\n",
    "#                                 prefix wd: <http://www.wikidata.org/entity/>\n",
    "\n",
    "#                                 SELECT ?res\n",
    "#                                 WHERE\n",
    "#                                 {{\n",
    "#                                 wd:{r} rdfs:label ?res .\n",
    "#                                 FILTER(LANG(?res) = \"en\").\n",
    "#                                 }}'''\n",
    "#                         kg_res.extend([str(i[0]) for i in set(graph.query(query))])\n",
    "                    \n",
    "#                     if kg_res and cs_ans:\n",
    "#                         if kg_res == cs_ans:\n",
    "#                             if cs_state == 'CORRECT' and cs_rate > 0.5:\n",
    "#                                 final_ans += f\" The {relation['relation']} of {ent_print} is {kg_res} (Crowd Approval rate: {cs_rate}).\"\n",
    "#                             else:\n",
    "#                                 final_ans += f\" I think the {relation['relation']} of {ent_print} is {kg_res} but im not really sure (Crowd Approval rate: {cs_rate}).\"\n",
    "#                         else:\n",
    "#                             if cs_state == 'CORRECT' and cs_rate > 0.5:\n",
    "#                                 final_ans += f\" The {relation['relation']} of {ent_print} is {cs_ans} (Crowd Approval rate: {cs_rate}).\"\n",
    "#                             else:\n",
    "#                                 final_ans += f\" I think the {relation['relation']} of {ent_print} is {kg_res} but im not really sure (Crowd Approval rate: {cs_rate}).\"\n",
    "#                     elif cs_ans:\n",
    "#                         if cs_state == 'CORRECT' and cs_rate > 0.5:\n",
    "#                             final_ans += f\" The {relation['relation']} of {ent_print} is {cs_ans} (Crowd Approval rate: {cs_rate}).\"\n",
    "#                         else:\n",
    "#                             final_ans += f\" I don't really know the {relation['relation']} of {ent_print}. But {cs_ans} is not (Crowd Approval rate: {cs_rate}).\"\n",
    "#                     elif kg_res:\n",
    "#                         if len(kg_res) > 1:\n",
    "#                             final_ans += f\" The {inflect_engine.plural_noun(relation['relation'])} of {ent_print} are {', '.join(kg_res)}.\"\n",
    "#                         else:\n",
    "#                             final_ans += f\" The {relation['relation']} of {ent_print} is {kg_res[0]}.\"\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "#                     #embedding\n",
    "#                     n_to_retr = len(kg_res)\n",
    "\n",
    "#                     emb_res = embeddings(entity_emb, ent2id, relation_emb, rel2id, ee['id'], rid, n_to_retr+1)\n",
    "\n",
    "#                     if emb_res:\n",
    "#                         if len(emb_res) > 1:\n",
    "#                             labels = ', '.join([e['label'] for e in emb_res])\n",
    "#                             scores = ', '.join([str(e['Score']) for e in emb_res])\n",
    "#                             final_ans += f\" The Embeddings suggest that the {inflect_engine.plural_noun(relation['relation'])} of {ent_print} could be {labels} (Scores: {scores}).\"\n",
    "#                         else:\n",
    "#                             final_ans += f\" The Embeddings suggest that the {relation['relation']} of {ent_print} coulb be {emb_res[0]['label']} (Score: {emb_res[0]['Score']}).\"\n",
    "\n",
    "                    \n",
    "# final_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "56c4ac18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #knowledge graph\n",
    "# kg = {}\n",
    "# for e in ent.values():\n",
    "#     for ee in e:\n",
    "#         kg[ee['entity']] = {}\n",
    "#         for relation in rel:\n",
    "#             kg[ee['entity']][relation['relation']] = []\n",
    "#             for rid in relation['ids']:\n",
    "#                 g = list(graph.objects(WD[ee['id']], WDT[rid]))\n",
    "\n",
    "\n",
    "#                 result = _EntityURI_to_ID(g, WD)\n",
    "#                 for r in result:\n",
    "#                     query = f'''\n",
    "#                             prefix wdt: <http://www.wikidata.org/prop/direct/>\n",
    "#                             prefix wd: <http://www.wikidata.org/entity/>\n",
    "\n",
    "#                             SELECT ?res\n",
    "#                             WHERE\n",
    "#                             {{\n",
    "#                             wd:{r} rdfs:label ?res .\n",
    "#                             FILTER(LANG(?res) = \"en\").\n",
    "#                             }}'''\n",
    "\n",
    "#                     kg[ee['entity']][relation['relation']].extend([str(i[0]) for i in set(graph.query(query))])\n",
    "# kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f66be769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # embeddings\n",
    "# import re\n",
    "# embd = {}\n",
    "# for e in ent.values():\n",
    "#     for ee in e:\n",
    "#         embd[ee['entity']] = {}\n",
    "#         for relation in rel:\n",
    "#             embd[ee['entity']][relation['relation']] = []\n",
    "#             for rid in relation['ids']:\n",
    "                \n",
    "#                 n_to_retr = len(kg[ee['entity']][relation['relation']])\n",
    "                \n",
    "#                 result = embeddings(entity_emb, ent2id, relation_emb, rel2id, ee['id'], rid, n_to_retr+1)\n",
    "\n",
    "\n",
    "#                 embd[ee['entity']][relation['relation']].extend(result)\n",
    "# embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaf6a81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1644cdb",
   "metadata": {},
   "source": [
    "# Dictionary loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bc75196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "import numpy as np\n",
    "import csv\n",
    "RDFS = rdflib.namespace.RDFS\n",
    "WDT = rdflib.Namespace('http://www.wikidata.org/prop/direct/')\n",
    "WD = rdflib.Namespace('http://www.wikidata.org/entity/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "49f5ae11",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_emb = np.load('Data/ddis-graph-embeddings/entity_embeds.npy')\n",
    "relation_emb = np.load('Data/ddis-graph-embeddings/relation_embeds.npy')\n",
    "\n",
    "# load the dictionaries\n",
    "with open('Data/ddis-graph-embeddings/entity_ids.del', 'r') as ifile:\n",
    "    ent2id = {rdflib.term.URIRef(ent): int(idx) for idx, ent in csv.reader(ifile, delimiter='\\t')}\n",
    "    id2ent = {v: k for k, v in ent2id.items()}\n",
    "with open('Data/ddis-graph-embeddings/relation_ids.del', 'r') as ifile:\n",
    "    rel2id = {rdflib.term.URIRef(rel): int(idx) for idx, rel in csv.reader(ifile, delimiter='\\t')}\n",
    "    id2rel = {v: k for k, v in rel2id.items()}\n",
    "\n",
    "ent2lbl = {ent: str(lbl) for ent, lbl in graph.subject_objects(RDFS.label)}\n",
    "lbl2ent = {lbl: ent for ent, lbl in ent2lbl.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "161db370",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_dict = {\n",
    "    'drama film': {'words': ['drama'], 'id': 'Q130232'},\n",
    "    'documentary film': {'words': ['documentary', 'factual'], 'id': 'Q93204'},\n",
    "    'comedy film': {'words': ['funny', 'comedy', 'comedic'], 'id': 'Q157443'},\n",
    "    'crime film': {'words': ['crime'], 'id': 'Q959790'},\n",
    "    'action film': {'words': ['action'], 'id': 'Q188473'},\n",
    "    'romance film': {'words': ['romantic', 'romance'], 'id': 'Q1054574'},\n",
    "    'horror film': {'words': ['horror', 'scary'], 'id': 'Q200092'},\n",
    "    'adventure film': {'words': ['adventure'], 'id': 'Q319221'},\n",
    "    'neo-noir': {'words': ['neo-noir', 'new-black', 'neo noir', 'new black'], 'id': 'Q2421031'},\n",
    "    'science fiction': {'words': ['science fiction', 'SF', 'scifi', 'sci Fi', 'fantasy' \n",
    "                                  'sci-Fi', 'science-fiction', 'sci fi', 'sciencefiction'], 'id': 'Q24925'},\n",
    "    'thriller film': {'words': ['thriller', 'suspense'], 'id': 'Q2484376'},\n",
    "    'animated film': {'words': ['animated', 'animation', 'cartoon'], 'id': 'Q202866'},\n",
    "\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dc66f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09494c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6fa2a0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Clean crowd data and rates...\n"
     ]
    }
   ],
   "source": [
    "from intent_decider import *\n",
    "intent_dec = IntentionDecider()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4d2702",
   "metadata": {},
   "source": [
    "# Run AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8b222313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "who is the director Good Will Hunting.\n",
      "\n",
      "NER\n",
      "['who is the director ', '']\n",
      "['Good Will Hunting']\n",
      "\n",
      "POS\n",
      "['who', 'is', 'the', 'director', 'Good', 'Will', 'Hunting', '.']\n",
      "['WP', 'VBZ', 'DT', 'NN', 'NNP', 'NNP', 'NNP', '.']\n",
      "\n",
      "{'WP': ['who'], 'VBZ': ['is'], 'DT': ['the'], 'NN': ['director'], '.': ['.']}\n",
      "\n",
      "[{'relation': 'director', 'ids': ['P57']}]\n",
      "{'MISC': [{'entity': 'Good Will Hunting', 'id': 'Q193835'}]}\n"
     ]
    }
   ],
   "source": [
    "testsent = \"who is the director Good Will Hunting.\"\n",
    "\n",
    "testsent = re.sub('executive producer', 'showrunner', testsent)\n",
    "testsent = re.sub('production designer', 'designer', testsent)\n",
    "testsent = re.sub('costume designer', 'costume', testsent)\n",
    "testsent = re.sub('box office', 'box', testsent)\n",
    "testsent = re.sub('narrative location', 'nlocation', testsent)\n",
    "testsent = re.sub('filming location', 'flocation', testsent)\n",
    "testsent = re.sub('production company', 'company', testsent)\n",
    "\n",
    "\n",
    "# testsent = re.sub(',', ' , ', testsent)\n",
    "print(testsent)\n",
    "\n",
    "entities,  Owords = ner_extractor.get_entities(testsent)\n",
    "\n",
    "pos = pos_extractor.get_pos(testsent)\n",
    "\n",
    "\n",
    "ent = ner_extractor.getEntities_URIIDs(graph, entities, WDT, WD, category2URIID)\n",
    "rel = pos_extractor.get_relations(pos, Owords, graph, WDT, film_properties)\n",
    "print()\n",
    "print(rel)\n",
    "print(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "746bada4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The director of Good Will Hunting (1997) is Gus Van Sant. The Embeddings suggest that the directors of Good Will Hunting (1997) could be Harmony Korine, Ben Affleck (Scores: 2721.4583, 2832.0962).'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intent_dec.decider(graph, WD, WDT, ent, rel, Owords, entity_emb, ent2id, ent2lbl, id2ent, \n",
    "                   relation_emb, rel2id, images, genre_dict, category2URIID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c1a370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a07baf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408dcbbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c89f4c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = (\"Sorry mate, couldn't get you or an answer, \" +\n",
    "\"I just respond to stuff about movies. By the way in case it is my fault, \" +\n",
    "\"make sure you spell correctly the entities (Case sensitive aswell) and \" +\n",
    "\"what you want because I forgot to learn magic \" +\n",
    "\"sorceries for spell correction. Better luck next time!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a579042a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sorry mate, couldn't get you or an answer, I just respond to stuff about movies. By the way in case it is my fault, make sure you spell correctly the entities (Case sensitive aswell) and what you want because I forgot to learn magic sorceries for spell correction. Better luck next time!\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc29f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
